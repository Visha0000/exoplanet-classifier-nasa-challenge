{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf88c091-fe43-4d53-b496-90444b5a5308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 00:27:43.578 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-10-04 00:27:43.586 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.103 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\user\\anaconda3\\envs\\VISHALAKSHI\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-04 00:27:44.105 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.106 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.108 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.111 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.134 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.135 Session state does not function when running a script without `streamlit run`\n",
      "2025-10-04 00:27:44.137 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.139 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.140 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.142 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-04 00:27:44.144 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# Sample CSV for fallback\n",
    "SAMPLE_CSV = \"\"\"kepid,kepoi_name,kepler_name,koi_disposition,koi_pdisposition,koi_score,koi_fpflag_nt,koi_fpflag_ss,koi_fpflag_co,koi_fpflag_ec,koi_period,koi_period_err1,koi_period_err2,koi_time0bk,koi_time0bk_err1,koi_time0bk_err2,koi_impact,koi_impact_err1,koi_impact_err2,koi_duration,koi_duration_err1,koi_duration_err2,koi_depth,koi_depth_err1,koi_depth_err2,koi_prad,koi_prad_err1,koi_prad_err2,koi_teq,koi_teq_err1,koi_teq_err2,koi_insol,koi_insol_err1,koi_insol_err2,koi_model_snr,koi_tce_plnt_num,koi_tce_delivname,koi_steff,koi_steff_err1,koi_steff_err2,koi_slogg,koi_slogg_err1,koi_slogg_err2,koi_srad,koi_srad_err1,koi_srad_err2,ra,dec,koi_kepmag\n",
    "10797460,K00752.01,Kepler-227 b,CONFIRMED,CANDIDATE,1.0000,0,0,0,0,9.488035570,2.7750000e-05,-2.7750000e-05,170.5387500,2.160000e-03,-2.160000e-03,0.1460,0.3180,-0.1460,2.95750,0.08190,-0.08190,6.1580e+02,1.950e+01,-1.950e+01,2.26,2.600e-01,-1.500e-01,793.0,,,93.59,29.45,-16.65,35.80,1,q1_q17_dr25_tce,5455.00,81.00,-81.00,4.467,0.064,-0.096,0.9270,0.1050,-0.0610,291.934230,48.141651,15.347\n",
    "10797460,K00752.02,Kepler-227 c,CONFIRMED,CANDIDATE,0.9690,0,0,0,0,54.418382700,2.4790000e-04,-2.4790000e-04,162.5138400,3.520000e-03,-3.520000e-03,0.5860,0.0590,-0.4430,4.50700,0.11600,-0.11600,8.7480e+02,3.550e+01,-3.550e+01,2.83,3.200e-01,-1.900e-01,443.0,,,9.11,2.87,-1.62,25.80,2,q1_q17_dr25_tce,5455.00,81.00,-81.00,4.467,0.064,-0.096,0.9270,0.1050,-0.0610,291.934230,48.141651,15.347\n",
    "10811496,K00753.01,,CANDIDATE,CANDIDATE,0.0000,0,0,0,0,19.899139950,1.4940000e-05,-1.4940000e-05,175.8502520,5.810000e-04,-5.810000e-04,0.9690,5.1260,-0.0770,1.78220,0.03410,-0.03410,1.0829e+04,1.710e+02,-1.710e+02,14.60,3.920e+00,-1.310e+00,638.0,,,39.30,31.04,-10.49,76.30,1,q1_q17_dr25_tce,5853.00,158.00,-176.00,4.544,0.044,-0.176,0.8680,0.2330,-0.0780,297.004820,48.134129,15.436\n",
    "10848459,K00754.01,,FALSE POSITIVE,FALSE POSITIVE,0.0000,0,1,0,0,1.736952453,2.6300000e-07,-2.6300000e-07,170.3075650,1.150000e-04,-1.150000e-04,1.2760,0.1150,-0.0920,2.40641,0.00537,-0.00537,8.0792e+03,1.280e+01,-1.280e+01,33.46,8.500e+00,-2.830e+00,1395.0,,,891.96,668.95,-230.35,505.60,1,q1_q17_dr25_tce,5805.00,157.00,-174.00,4.564,0.053,-0.168,0.7910,0.2010,-0.0670,285.534610,48.285210,15.597\n",
    "10854555,K00755.01,Kepler-664 b,CONFIRMED,CANDIDATE,1.0000,0,0,0,0,2.525591777,3.7610000e-06,-3.7610000e-06,171.5955500,1.130000e-03,-1.130000e-03,0.7010,0.2350,-0.4780,1.65450,0.04200,-0.04200,6.0330e+02,1.690e+01,-1.690e+01,2.75,8.800e-01,-3.500e-01,1406.0,,,926.16,874.33,-314.24,40.90,1,q1_q17_dr25_tce,6031.00,169.00,-211.00,4.438,0.070,-0.210,1.0460,0.3340,-0.1330,288.754880,48.226200,15.509\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Data Loading & Preprocessing Function\n",
    "@st.cache_data\n",
    "def load_and_preprocess_data(file_path_or_str):\n",
    "    try:\n",
    "        if isinstance(file_path_or_str, str) and not file_path_or_str.startswith('http'):\n",
    "            # Skip comment lines and set delimiter explicitly\n",
    "            df = pd.read_csv(file_path_or_str, comment='#', sep=',', on_bad_lines='skip')\n",
    "        else:\n",
    "            df = pd.read_csv(io.StringIO(file_path_or_str), comment='#', sep=',', on_bad_lines='skip')\n",
    "        \n",
    "        # Select relevant features\n",
    "        feature_cols = ['koi_period', 'koi_prad', 'koi_depth', 'koi_duration', 'koi_impact', 'koi_teq', 'koi_insol']\n",
    "        available_cols = [col for col in feature_cols if col in df.columns]\n",
    "        df = df[available_cols + ['koi_disposition']].dropna(subset=available_cols + ['koi_disposition'])\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            st.error(\"No valid data after preprocessing. Check CSV format.\")\n",
    "            st.stop()\n",
    "        \n",
    "        # Encode target\n",
    "        le = LabelEncoder()\n",
    "        df['label'] = le.fit_transform(df['koi_disposition'])\n",
    "        \n",
    "        X = df[available_cols].values\n",
    "        y = df['label'].values\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test, scaler, le, available_cols, df['koi_disposition'].value_counts()\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading CSV: {e}. Using sample data.\")\n",
    "        df = pd.read_csv(io.StringIO(SAMPLE_CSV), comment='#', sep=',', on_bad_lines='skip')\n",
    "        feature_cols = ['koi_period', 'koi_prad', 'koi_depth', 'koi_duration', 'koi_impact', 'koi_teq', 'koi_insol']\n",
    "        available_cols = [col for col in feature_cols if col in df.columns]\n",
    "        df = df[available_cols + ['koi_disposition']].dropna(subset=available_cols + ['koi_disposition'])\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        df['label'] = le.fit_transform(df['koi_disposition'])\n",
    "        \n",
    "        X = df[available_cols].values\n",
    "        y = df['label'].values\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test, scaler, le, available_cols, df['koi_disposition'].value_counts()\n",
    "\n",
    "# Step 2: PyTorch MLP Model\n",
    "class ExoplanetClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ExoplanetClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def train_model(X_train, y_train, input_size, epochs=50, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ExoplanetClassifier(input_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model.to('cpu'), device\n",
    "\n",
    "# Step 3: Evaluation Function\n",
    "def evaluate_model(model, X_test, y_test, le):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.tensor(X_test, dtype=torch.float32))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        acc = accuracy_score(y_test, predicted.numpy())\n",
    "        report = classification_report(y_test, predicted.numpy(), target_names=le.classes_)\n",
    "        cm = confusion_matrix(y_test, predicted.numpy())\n",
    "    return acc, report, cm\n",
    "\n",
    "# Step 4: Confusion Matrix Plot\n",
    "def plot_confusion_matrix(cm, le_classes):\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=le_classes, yticklabels=le_classes,\n",
    "           title='Confusion Matrix',\n",
    "           ylabel='True label', xlabel='Predicted label')\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Step 5: Streamlit UI\n",
    "def main():\n",
    "    st.title(\"Exoplanet Classifier: NASA Space Apps Challenge\")\n",
    "    st.write(\"Upload Kepler/TESS CSV or enter features to classify as CONFIRMED, CANDIDATE, or FALSE POSITIVE.\")\n",
    "    \n",
    "    # Load data\n",
    "    uploaded_file = st.file_uploader(\"Upload CSV (uses sample if none)\", type='csv')\n",
    "    if uploaded_file is not None:\n",
    "        csv_content = uploaded_file.getvalue().decode()\n",
    "        file_path = csv_content\n",
    "    else:\n",
    "        file_path = r\"C:\\Users\\user\\Desktop\\Nasa\\cumulative_2025.10.03_08.57.32.csv\"\n",
    "    \n",
    "    if st.button(\"Load & Train Model\"):\n",
    "        with st.spinner(\"Preprocessing and training...\"):\n",
    "            try:\n",
    "                X_train, y_train, X_test, y_test, scaler, le, feature_cols, class_dist = load_and_preprocess_data(file_path)\n",
    "                input_size = len(feature_cols)\n",
    "                model, _ = train_model(X_train, y_train, input_size)\n",
    "                \n",
    "                # Evaluate\n",
    "                acc, report, cm = evaluate_model(model, X_test, y_test, le)\n",
    "                st.metric(\"Test Accuracy\", f\"{acc:.2%}\")\n",
    "                st.text(\"Classification Report:\\n\" + report)\n",
    "                \n",
    "                # Plot Confusion Matrix\n",
    "                fig = plot_confusion_matrix(cm, le.classes_)\n",
    "                st.pyplot(fig)\n",
    "                \n",
    "                # Cache\n",
    "                st.session_state.model = model\n",
    "                st.session_state.scaler = scaler\n",
    "                st.session_state.le = le\n",
    "                st.session_state.feature_cols = feature_cols\n",
    "                st.session_state.X_train = X_train\n",
    "                st.session_state.y_train = y_train\n",
    "                st.session_state.input_size = input_size\n",
    "                st.session_state.class_dist = class_dist\n",
    "                \n",
    "                st.success(\"Model trained! Class distribution:\\n\" + class_dist.to_string())\n",
    "            except Exception as e:\n",
    "                st.error(f\"Training failed: {e}\")\n",
    "    \n",
    "    # Prediction Section\n",
    "    if 'model' in st.session_state:\n",
    "        st.subheader(\"Predict New Data\")\n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            st.write(\"Manual Entry:\")\n",
    "            inputs = {}\n",
    "            for col in st.session_state.feature_cols:\n",
    "                inputs[col] = st.number_input(f\"{col} (default: 0)\", value=float(0.0), key=col)\n",
    "            if st.button(\"Predict Single\"):\n",
    "                new_data = np.array(list(inputs.values())).reshape(1, -1)\n",
    "                new_scaled = st.session_state.scaler.transform(new_data)\n",
    "                with torch.no_grad():\n",
    "                    output = st.session_state.model(torch.tensor(new_scaled, dtype=torch.float32))\n",
    "                    _, pred = torch.max(output, 1)\n",
    "                    prob = torch.softmax(output, dim=1).numpy()[0]\n",
    "                pred_label = st.session_state.le.inverse_transform([pred.item()])[0]\n",
    "                st.success(f\"Prediction: **{pred_label}**\")\n",
    "                st.write(\"Probabilities:\")\n",
    "                for i, p in enumerate(prob):\n",
    "                    st.write(f\"- {st.session_state.le.classes_[i]}: {p:.2%}\")\n",
    "        \n",
    "        with col2:\n",
    "            new_file = st.file_uploader(\"Upload New Data CSV\", key=\"new\")\n",
    "            if new_file and st.button(\"Predict Batch\"):\n",
    "                try:\n",
    "                    new_csv = new_file.getvalue().decode()\n",
    "                    new_df = pd.read_csv(io.StringIO(new_csv), comment='#', sep=',', on_bad_lines='skip')\n",
    "                    available_cols = [col for col in st.session_state.feature_cols if col in new_df.columns]\n",
    "                    new_X = new_df[available_cols].fillna(0).values\n",
    "                    new_scaled = st.session_state.scaler.transform(new_X[:, :len(available_cols)])\n",
    "                    with torch.no_grad():\n",
    "                        outputs = st.session_state.model(torch.tensor(new_scaled, dtype=torch.float32))\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                    new_df['predicted_disposition'] = st.session_state.le.inverse_transform(preds.numpy())\n",
    "                    st.dataframe(new_df[['kepid', 'koi_disposition', 'predicted_disposition']].head() if 'kepid' in new_df else new_df.head())\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Batch prediction failed: {e}\")\n",
    "    \n",
    "    # Retrain Option\n",
    "    if st.checkbox(\"Retrain with Hyperparams\") and 'X_train' in st.session_state:\n",
    "        epochs = st.slider(\"Epochs\", 10, 100, 50)\n",
    "        lr = st.slider(\"Learning Rate\", 0.0001, 0.01, 0.001, 0.0001)\n",
    "        if st.button(\"Retrain\"):\n",
    "            with st.spinner(\"Retraining...\"):\n",
    "                model, _ = train_model(st.session_state.X_train, st.session_state.y_train, st.session_state.input_size, epochs, lr)\n",
    "                st.session_state.model = model\n",
    "                st.success(\"Model Retrained!\")\n",
    "                st.rerun()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
